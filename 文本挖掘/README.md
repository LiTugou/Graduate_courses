# 文本挖掘
## 基础文本表示
### 文本处理
分词 -> 词性标注 -> 依赖解析 -> 其它（时态，词干，停用词）
### 向量空间模型

用一组词表示一段文本，而不考虑语法或词语顺序

常用方法：

1. One-Hot
2. Term Frequency(TF)
3. Term Frequency Inversed Document Fequency(TF-IDF)
#### One-Hot

假设现有单词数量为N的词表，可以通过一个长度为N的向量来表示一个单词，在这个向量中该单词对应的位置数值为1，其余单词对应的位置数值全部为0。举例如下：

| Dict | queen | king | man | woman | boy | girl |
| --- | --- | --- | --- | --- | --- | --- |
| queen | 1 | 0 | 0 | 0 | 0 | 0 |
| king | 0 | 1 | 0 | 0 | 0 | 0 |
| boy | 0 | 0 | 0 | 0 | 1 | 0 |

缺点：

1. 矩阵稀疏
2. 无上下文信息
3. 不考虑词语位置
#### TF
$w$表示单词，$d$表示文本，$f(w,d)$是单词$w$在文本$d$中的频数。

$$
tf(w,d)=log(1+f(w,d))
$$

#### TF-IDF
D表示所有文本集合,$f(w,D)$表示单词$w$在$D$中的频数

$$
\begin{align}
idf(w,D)=log(\frac{N}{f(w,D)}) \\
tf-idf(w,d,D)=tf(w,d)\times idf(w,D)
\end{align}
$$

优点：
- 简单灵活  
缺点：

- Vocabulary：词表的设计要小心，特别是词表的大小
- Sparsity：难以计算和提取信息
- Meaning：没有考虑词语的顺序和上下文信息，也没有考虑词语在$D$中的意思
### 主题模型
#### LSA(潜在语义分析)

![](./images/LSA.png)

$$X\approx U_k\Sigma_k V_k^T$$

其中$dim(X)=m\times n$,m为词表长度，n为文本集合大小。上式为对$X$的截断奇异值分解（还可使用非负矩阵分解，总之就是分解为话题空间和文本在话题空间上的投影）。

- 话题空间$U_k$的每一个列向量表示一个话题，称为话题向量。
- $(\Sigma_kV_k^T)$为文本在话题空间的表示  
优点：
- low-dimensional space
- 无监督学习  
缺点：
- 计算复杂，有新文本加入时需要重新训练
- 没有明确的理论解释
#### pLSA(概率潜在语义分析)
优点：

- 建立了一个概率模型，有清晰的理论解释
- 解决了同义词和多义词的问题  
缺点：
- 随着文本和词语的增加，PLSA模型的参数线性增长
- 但它无法生成新的文档模型。我们不知道$p(d)$
- EM算法更加的计算密集
